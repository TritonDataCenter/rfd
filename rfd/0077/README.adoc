:author: Alex Wilson
:email: alex.wilson@joyent.com
:state: draft
:revremark: State: {state}

:showtitle:
:toc: left
:numbered:
:icons: font

////
    This Source Code Form is subject to the terms of the Mozilla Public
    License, v. 2.0. If a copy of the MPL was not distributed with this
    file, You can obtain one at http://mozilla.org/MPL/2.0/.

    Copyright 2017 Joyent Inc
////

# RFD 77 Hardware-backed per-zone crypto tokens

## Introduction

While most existing access control for Triton and Manta focusses on
authenticating *users* and authorizing their actions, there are quite a
few contexts in which authenticating individual instances (zones) on the system
is useful.

This particularly applies in the context of core service zones in Triton,
where it would be desirable for each service to be able to authenticate itself
to any other service, giving a path forward to eliminate the special "trusted"
status of all hosts on the admin VLAN.

This kind of authentication is generally most easily done through cryptography,
which is what this document will propose.

To store cryptographic key material securely with a machine, in a way that is
highly resistant to compromise, the state of the art is to make use of a
segregated hardware device (a "token"). The token stores the key material
internally and will not reveal it to the host (and is generally a physically
tamper-resistant device, making it extremely difficult to recover the key
material from it without destroying it). The host machine may request the token
to take particular actions with the key material, such as signing a given
stream of data, encrypting a block of data, or computing a hash function.

This kind of secured credential storage would be useful not just for
authentication, but also for the protection of data at rest. As a result,
this proposal also includes provisions for supporting this use case (though
implementing the on-disk data encryption is delegated to ZFS).

[[defns]]
## Definitions

CN:: A "CN" here is any physical node in the Triton system.
Trusted CN:: A "trusted" CN is one that is trusted to run components of the
Triton system itself (e.g. VMAPI, NAPI, CloudAPI etc)

Self-booting CN:: A "self-booting" CN is one that can boot entirely standalone,
without any other machine in the datacenter or network access being available.

Headnode:: A "headnode" is a term for a CN that is both trusted and
self-booting. Any Triton datacenter needs to have at least one such node.

Core service:: A "core service" is an internal Triton service which runs in a
zone, such as VMAPI or CNAPI. These are often referred to as "headnode services"
today, but we will avoid that term here to reduce confusion with the last
definition.

ECDH:: We will use the term "ECDH" to refer to elliptic-curve Diffie-Hellman key
agreement protocol, as specified by NIST SP 800-56A (specifically the EC
cofactor/CDH variant of the protocol). It is normally implied that the output of
the protocol is processed by a cryptographic hash function before further use.

DH box:: The term "Diffie-Hellman box" or "DH box" is used here to refer to an
encrypted container for data, encrypted and authenticated using a symmetric key
derived by performing ECDH. One of the key pairs involved in the ECDH protocol
for a box is ephemeral, while the other is static. The private half of the
ephemeral key pair is securely discarded after performing ECDH. Such a box can
be constructed by anyone in possession of the recipient's public key, but can
only be decrypted by the owner of the private key.

## Threat models and goals

### Authentication

The core threat models (and containment goals) for the authentication scheme
proposed here are as follows:

Threat 1:: An intruder has escalated their network access into the admin VLAN, by
compromise or poor configuration of network equipment or non-Triton
resources.
Goal 1:: The goal is to give this attacker no access to Triton resources. They
may make a read-only request for a boot image for a new CN which will contain
no special credentials, but no more. They may be able to carry out denial of
service attacks on the admin VLAN, but these are out of scope for this design.

Threat 2:: An intruder has escalated their network access into the admin VLAN, by
compromise of an ordinary (not "trusted") compute node (privilege escalation and
zone escape).
Goal 2:: The goal is to give this attacker only the minimum access required for
the normal operation of the CN. They will be able to control other zones on that
CN, as well as the information reported about them back to the rest of Triton.
They will under no circumstances be able to gain control of a trusted CN from
this position. Their access to the system can be terminated by revoking the
credentials of the CN, they cannot extract any long-lived key material, and
cannot take any actions that would escalate or allow sideways movement into
other CNs.

Threat 3:: An intruder has taken control of a public-facing core service
(e.g. CloudAPI), by making use of a vulnerability in that service.
Goal 3:: The goal is to give this attacker only the minimum access required by
the normal operation of that service. This means, for example, that CloudAPI
would not be able to run arbitrary commands on CNs or directly interface with
CN agents, or connect directly to the PostgreSQL database (since such access
is not needed for its normal operation).

### Encryption at rest

For the encryption of data at rest, the primary threat model is as follows:

Threat 1:: An intruder gains physical possession of disks and/or hardware
from a CN, either by post-disposal acquisition ("dumpster diving"), or outright
physical theft.
Goal 1:: The goal is to give the attacker no ability to read any customer data
on the disks or (in the case of a disposed CN) any ability to use the
credentials of the CN to gain access to Triton resources. If a stolen CN is
powered up at the time of theft, it is possible that customer data can be read,
but if powered down, no data access will be possible.

[[customer-features]]
### Customer-facing features

This design also seeks to provide 4 key customer-facing features:

Feature 1:: The ability to use a provisioned instance/zone/VM in a customer
account as an authentication principal to Triton (and other Triton-aware)
services.
Goal 1:: The credentials of this principal should not be able to be permanently
compromised by an attacker who has full control of a customer zone (i.e. they
must not be able to access key material).

Feature 2:: The ability to have customer-provisioned instances authenticate
to each other (both within a datacentre and between them) using credentials
provided by Triton itself.
Goal 2:: The credentials used for this authentication should not be able to be
permanently compromised by an attacker who has full control of a customer zone.

Feature 3:: The ability to implement a secure data store protected by hardware
symmetric keys within a zone.
Goal 3:: If an attacker compromises a customer zone storing N items of data
protected by this mechanism, they should have no choice but to make N individual
round trips through a (rate-limited) hardware module in order to decrypt them.
If the attacker compromises an entire live Triton CN (including the contents of
RAM) with M zones on it, they should have no choice but to make at least M round
trips through a hardware module (or perform computation taking at least as long)
in order to access customer data so protected.

NOTE:: Goal 3 explicitly does not include absolute defense of this data against
an attacker who has complete control of the OS kernel for an abitrarily long
period. It *does*, however, set a minimum amount of time an attacker must be
present with such control in order to break the security of protected storage on
the machine: the attacker must spend at least as long there as it would take to
make N trips through the hardware module.

Feature 4:: The ability to provision instances onto encrypted datastores.
Goal 4:: It is not required that every CN in a Triton install have encrypted
local storage (though having all CNs use encrypted zpools is certainly an
allowed configuration).  If a customer decides that an instance will contain
information that must be protected while at rest (i.e. encrypted), they should
be able to guarantee that such instances are either provisioned on a CN with an
encrypted zpool or the provision request fails if it is not possible to meet
the encryption requirement -- it is essential that an instance requesting
encryption is _never_ allowed to provision onto an unencrypted CN.

## Design

The central component of the design is the credential storage device. Since
many components of our threat model and goals are on a per-CN basis, we want a
device that can be deployed with (or ideally, inside) every CN. This implies
that:

 * The device must be inexpensive (at least, relative to expected cost of CN
   hardware);
 * The device must be capable of storing credentials both for at-rest encryption
   and for authentication; and
 * The device must not require invasive modification to current-generation
   x86 server hardware.

Most commonly, cryptographic token devices obey an API similar to PKCS#11, which
is primarily focussed on public/private asymmetric cryptography. Devices that
only implement asymmetric cryptography are suitable for storing authentication
credentials, but do not always fit as well in a design that wants to store
credentials for at-rest encryption. A notable exception is devices that support
a key agreement scheme like Diffie-Hellman using their private key material,
which can be used with an ephemeral keypair to form a Diffie-Hellman "box".

In hardware there are always difficult trade-offs between price, features, and
performance. What is implicit in the above list of goals is that the
cryptographic performance of the device is likely to be low (as it is both
cheap and well-featured). As a result, the rate at which hardware operations
need to take place must to be limited in the system design.

One device that is suited for these goals is the Yubikey (manufactured by
Yubico). It implements a number of features aimed at the 2-factor Authentication
market (based on hash chains and HMAC) which are also ideal for securely
deriving encryption keys. Alongside these features, it features RSA and ECDSA
asymmetric cryptography, both for signature operations and key agreement.

The Yubikey is relatively inexpensive (at $40 US it is a very small line item in
the typical cost of a new CN), and since it uses the ubiquitious USB interface
it can easily be added to existing server hardware (in fact, many servers
include USB connectors that are located inside the server casing which are
ideal locations for this use).

Alternatives to the Yubikey that are also well suited include a few models of
USB JavaCard tokens, such as the Feitian eJava token (also sold as the PIVKey
T800). These tokens can be written with appropriate JavaCard Applets to become a
drop-in replacement for the Yubikey (exposing the same commands to the server).

The hardware details of these devices and the interfaces they expose is
discussed further in the section <<hardware>>.

[[at-rest]]
### Overview: at-rest encryption

The concept for at-rest encryption is to use a randomly-generated key, and then
to protect it cryptographically such that 3 pieces of information are needed to
recover it:

 * A private key generated on the hardware token (which it will not reveal);
 * A randomly generated secret PIN stored on a trusted node service in the
   datacenter; and
 * The encrypted copy of some random data, stored as a ZFS pool property.

In this way, a node's disks cannot be decrypted unless an attacker has all three
of:

 * The disks belonging to the node;
 * The cryptographic token belonging to the node; and
 * Access to the PIN stored in the core service.

The primitive used to create these properties is the elliptic curve Diffie-
Hellman key agreement protocol (ECDH). Setting up the pool proceeds as follows:

 1. Generate a random byte string.
 2. Create a DH box (see <<defns>>) that can only be decrypted using the
    hardware token's private key. Place the random byte string in it.
 3. Place the encrypted data from the DH box in a ZFS pool property along with
    the public key of the ephemeral keypair.
 4. Use the byte strings as the ZFS encryption master key.

The private key in the hardware token is protected by a PIN -- a 10 digit
numeric code that must be provided to the token before any activity involving
the key is permitted. After 5 failed attempts at the PIN (and an additional 3
attempts at a PUK), the hardware token erases its keys. This PIN code is
stored in a Triton core service and is unique for each hardware token.

In order to re-derive the ZFS encryption master key for this node again on a
subsequent boot, we will have to send this PIN back to the hardware token,
perform ECDH with the hardware token's private key, then decrypt the DH box to
obtain the byte string from step 1 again to unlock the ZFS pool.

A single master key will be used for the whole pool, rather than a key per zone
or per customer. The current ZFS encryption design does not allow clones of ZFS
datasets to cross a key boundary, and since Triton relies heavily on zones being
able to be clones of their image datasets, making a separate key spaces is
impractical. Additionally, in the current Triton design, CNs are the source of
truth about what zones run on them (and changing that here is out of scope), so
there is little benefit in using a finer-grained scheme.

This approach has two major issues, however: firstly is the case of a headnode.
A Triton headnode, as defined earlier, must be able to boot from its own media,
without requiring the rest of the surrounding DC to be running (as it may be
hosting the PXE DHCP server that allows other non-self-booting CNs to boot).

As a result, self-booting nodes will not use a remotely stored PIN. They will
have the PIN code for their tokens either stored in USB flash media, or provided
at every boot on the console (for environments where cold-theft security is more
important than unattended reboot). This means that self-booting nodes do not
meet the full goal discussed above -- the theft of an entire working headnode
will allow that headnode's disks to be read.

This is a difficult compromise between fault tolerance, ability to boot the
whole DC up after power loss, and security. It may be worthwhile to examine
the possibility of special physical security measures to protect headnodes
beyond those used for ordinary non-headnode CNs. As there is normally a
small number of headnodes, this is at least more feasible than such protections
for the entire server population.

The second major issue is durability, or ability to recover from the failure of
a node's hardware crypto token. Clearly it would be undesirable to create a
single point of hardware failure that results in all data on the node being
irretrievable. As a result, an additional step is added where as well as
creating an encrypted DH box keyed to the hardware token for that CN, we create
a second box keyed to a set of offline "recovery keys" for the datacenter. The
public half of the recovery keys are distributed to all CNs for this purpose,
but the private half is kept in offline redundant secure storage to only be used
in emergencies. This is explored further in <<prov-backups>>.

### Overview: authentication

Authentication of a CN to a core service (e.g. to join the cluster, and
then to report data about running zones etc) is done by signing existing
protocol units (e.g. HTTP requests) using the asymmetric keys stored in the CN's
Yubikey. This is relatively straightforward.

Authentication of one core service zone to another is also done by signing
existing protocol units using asymmetric keys. Existing protocols in use between
core services are mostly variants of HTTP REST, and these will use the same HTTP
signature method used by public Triton APIs. Non-HTTP core services will be
expected to use TLS client certificates (the details of which will be explained
shortly).

Unfortunately, hardware tokens are generally only capable of storing a small
number of asymmetric keys, and the number of zones on a CN or headnode may be
quite large by comparison. The performance limitations of hardware tokens (given
the "inexpensive" price constraint we've already accepted) also mean that
scaling their usage up with the number of customer zones on a machine is likely
to be infeasible. So the keys used for zone-to-zone authentication cannot reside
directly on the hardware tokens.

Instead, a "soft token" design will be used. A randomly generated symmetric key
will be used to encrypt a keystore for that zone, and the key will be placed in
a DH box openable by the hardware token's private key. This keystore encryption
is always used, so that the same code path is taken on machines with and without
ZFS level storage encryption available.

The encrypted key store is managed by the global zone on behalf of the zones,
and exposed to them via a socket that processes in the zone can connect to. The
non-global zone cannot add or remove keys from the key store; it only holds a
fixed set of keys that the global zone has pre-generated and assigned to it.

The socket is designed to make use of the OpenSSH agent protocol. This protocol
is designed to be simple and straightforward to parse in a secure manner, and
since the SSH agent is more or less a "soft token" itself, an almost perfect
match for this use case.

The SSH agent also features support for SSH certificates, which can be used to
attest about an identity associated with a given key. The CN's global zone will
generate one such certificate for each zone and sign it using the same key it
uses for HTTP signature authentication. In this way, zones each have access to a
signed statement from their host CN about their identity, which they can use as
part of authentication.

A signed statement or certificate and a matching key is not enough on its own,
however, to validate the identity of one zone to another arbitrary zone on the
system -- the other zone needs to also be able to validate the key of the host
CN. To achieve this requires a chain of trust.

Agents running in the global zone of a CN are also expected to make use of a
soft-token instance for their routine work of signing core service requests. The
hardware tokens' workload will largely be limited to re-signing certificates for
each soft token periodically, and deriving keys for encryption at rest.

### Trusted CNs and chain of trust

As is typical with any chain of trust, we must begin with a set of keys known
as "root keys", which are ultimately trusted. What we propose here is to use
a single root key which is only ever stored offline, broken into pieces.

It is a key part of this design that the root key is not ever kept "on-line" in
the datacenter. If trusted CNs were ever given access to a secret like a root
key, and we ever needed to dispose of that trusted CN, we would be forced to
change the root key -- not just on that CN but on all CNs in the cluster. This
creates severe administrative burden which we seek here to avoid: disposing of
a trusted CN should not require revoking any credentials on *other* CNs.

This root key will sign an initial statement stating that certain nodes in the
cluster are to be Trusted CNs, detailing their public keys, as well as a
timestamp and serial number. It will then (barring exceptional circumstances)
never be used again.

To this statement, the Trusted CNs of the datacenter may append additional
statements, with certain restrictions:

 * Any appended statement must include a signature both over the new statement
   and all previous statements in the chain; and
 * The appended statement must be signed by the keys of all Trusted CNs in the
   datacenter at the time of appending, except one (N-1 out of N, unless there
   is only one Trusted CN at the time, in which case its signature is required
   footnoteref:[,It is also worth noting that with this rule, there is no real
   advantage to permanently having exactly 2 trusted CNs -- it will cost in
   terms of overhead without increasing security, since a single signature is
   still all that is required to update the trusted set.]).

The statement may declare that a new node (with corresponding key etc) is now
a Trusted CN, or it may declare that an existing Trusted CN is no longer such.

All CNs in the system (both regular and trusted) periodically gossip their
current version of the Trusted CN chain out over the network, to a multicast
address on the admin VLAN.

If a CN receives a new chain, it will accept it as the new canonical version
of the chain if and only if:

 * All signatures on the chain validate, including validation of the N-1/N
   restriction; and
 * The chain is a strict extension of the current canonical chain known to the
   CN; OR
 * The chain is an unrelated brand new chain, with a higher serial number and
   newer timestamp on the very first statement.

In this way, in an emergency situation, the chain can be restarted by using the
offine master key to sign a new statement about the Trusted CNs for the
installation.

This design allows Trusted CNs to be added and removed from the installation at
a later date without requiring that the root of the chain of trust be available
in online storage for signing.

Once the gossip process has stabilized, all CNs in the system are aware of the
identities and keys of nodes that are authorized to act as Trusted CNs (hosting
core Triton services). This means that zone certificates presented by zones on
these CNs can be validated, authenticating core services to each other.

It is important to note that changes to the set of Trusted CNs are expected to be
infrequent, so it is not important to use a distributed system here that offers
fast convergence. The simplicity of implementation of a gossip design is also
an advantage.

### TLS keys and Soft HSM

Aside from the main zone authentication key and its matching certificate, the
soft token stores two more keys on behalf of the non-global zone: a TLS
certificate signing key, and a symmetric key.

The TLS certificate signing key can only be used to sign X.509 certificates
about keys generated locally within the zone. A Triton-specific extension to the
SSH agent protocol allows for this, as well as the ability to request a
certificate chain.

The certificate chain consists of a set of X.509 certificates describing,
in order:

 1. A trusted head node in the datacentre (self-signed)
 2. The host CN of the zone (its hardware key, signed by the head node)
 3. The soft-token TLS signing key for the zone (signed by the host CN)

These certificates (both the TLS signing key for the zone and the chain
certificates, other than the head node) are limited to a very short window of
validity (60 seconds). The intention is that this chain can be obtained and used
only during an authentication process, and a fresh certificate obtained
regularly to repeat the operation as neeeded. There is no need to check with a
separate revocation list or manage one, as the short lifetime ensures that the
key in question is vouched for by the system: all that clients are required to
do is to keep their list of head node CA certificates up to date with the state
of the gossip engine.

The symmetric key stored in the soft token is treated differently to other keys
in token storage. It is not kept decrypted in memory in the soft token when not
in use; instead, a round trip through the system's hardware module must be made
for every use of this key. This also implies that access to this key is
rate-limited by the system to avoid users overburdening the hardware module.

Rather than encrypting material directly with this key, a data key scheme is
used. This means that each "encrypt" or "decrypt" request made to use this key
must be accompanied by an encrypted subkey. Inside the soft token, the subkey is
decrypted using the master key, which is then used to encrypt or decrypt the
actual data. This further limits the burden users may impose directly upon the
system's hardware module (by limiting the maximum amount of data that must be
transferred through the token itself).

An encrypted subkey ready for use may be obtained using a third operation
through the token interface. All 3 of these operations (encrypt, decrypt, and
generate subkey) are Triton-specific extensions to the SSH agent protocol.

The intention of the symmetric key capability is to enable the implementation
of systems that achieve the 3rd customer goal in <<customer-features>>.

### Binder and service registration

Having to make use of and validate full certificate chains for all traffic is
somewhat difficult to work into some existing systems within Triton. A simpler
proposition is to include only some form of key signature in these types of
traffic (e.g. by embedding it a legacy username and password) rather than a full
certificate.

To this end, `binder` (the Triton service discovery mechanism) will be altered,
such that clients can establish a trusted relationship with binder, and binder
can then take over the role of validating certificates on clients' behalf.

As the client half this relationship can be maintained from within a library
such as `cueball`, this will ease integration for core services -- they will
merely need to use the `cueball` library to manage their connections and will
then get identity validation on their outgoing connections "for free".

On the registration side of binder, registrants will be required to supply their
SSH certificate and public key along with the information they supply to binder
today (which will be signed with the key).

Binder will validate the signature and certificate provided, and then serve
DNS records about the registrant. These records will include public key records
containing the registered public key they supplied.

Traffic between binder and clients will be secured using the public-key
modification of DNS Transaction Signatures (TSIG) known as SIG(0) (RFC2931),
signed using the binder instance's zone key. The client must validate the binder
instance's key against its certificate and the gossiped list of Trusted CNs, but
thereafter it can trust signed responses from that binder about other services
in lieu of performing full validation itself.

The SIG(0) mechanism provides authentication of data in the DNS packet using a
cryptographic signature, but not confidentiality (the traffic is not encrypted).
As binder is not serving information that needs to be kept secret, this is a
suitable trade-off. It is transaction-oriented (signs the transactional message,
not just the data inside), relatively simple, requires minimal modification of
existing DNS software, is backwards-compatible and is also algorithm-agile
(allowing us to change the precise algorithm in use over time). For these
reasons, it is the proposed choice here over other alternative mechanisms like
DNSCurve or full DNSSEC.

Binder will also have to transition away from using the raw ZooKeeper direct
access for registration that it uses today, as the authentication schemes
available there will not be sufficient to ensure separation of clients.

[[prov-backups]]
### Provisioning and backups

When crypto tokens like the Yubikey are manufactured, they generally do not ship
with credentials pre-loaded on them (Yubikeys do in fact ship with some
basic credentials for the Yubico official 2FA, but this is not very useful
for our usecase). They have to be commanded to generate or write credentials
by an administrator who configures them before use.

Where possible, it is best for credentials to be generated on the token itself
(so that they never leave it and thus cannot be directly compromised). Keys
used for authentication or certificate signing can be replaced after a loss
by creating trust for a new set of keys instead, so there is no real need to
back them up.

Loss of at-rest encryption keys, on the other hand, leads to the loss of any
data protected by them (meaning loss of customer data). To guard against this
for the ZFS on-disk encryption keys, as explained earlier, we make use of a
scheme similar to key escrow, where a second DH box is created that enables the
retrieval of the ZFS encryption key using either the node's own key, *or* an
offline recovery key (or keys).

This recovery key, as well as the root key used to bootstrap the headnode chain
of trust, must be stored offline in a way that is both very secure and very
durable.

Keys may be split up into "pieces" for backup purposes, using secret-sharing
arrangements like Shamir's secret sharing. These enable schemes such as N out of
M piece secret recovery (while revealing no information in the case of fewer
pieces being held).

If the pieces are stored in separate geographic locations with separate access
controls, this can enable a form of the "2-person rule" (or "N-person rule") to
be enforced, where these valuable "master" keys can only be used with the
co-operation of multiple trusted members of the organization.

While the "root" key can truly be treated as an offline master key that is only
for serious (and rare) emergencies, hardware failures in a large datacentre are
a regular, expected event. As a result, the recovery keys must receive different
treatment for storage to enable efficient operation.

Our proposal is have hardware tokens assigned personally to trusted staff, have
these tokens generate a public-private EC key pair, and write the set of N
public keys for all of them into the Trusted CN chain as a separate kind of
chain entry that has to be signed by all current Trusted CNs.

Then, on each CN we take the symmetric disk encryption key and split it into
N Shamir pieces. Each of these pieces is then placed in an ECDH box targetting
one of the public keys registered in the lastest backup instruction entry in
the chain.

During recovery, we perform a challenge-response procedure (detailed later,
designed to resist replay attacks and not reveal the key if exposed) using these
ECDH boxes with the remote hardware tokens to reconstruct the original symmetric
key from the decrypted pieces in memory.

As individuals come and go from this set, a new recovery key chain entry will
be written and signed by the Trusted CNs. Then, all other CNs will regenerate
their Shamir pieces and ECDH boxes from scratch with the new set of public
keys.

The chain entry can also specify the number of the N pieces that will be
required for recovery, so that it can be changed if the group shrinks or
expands.

In summary:

 * Generation and preparation of the root key will take place in an environment
   away from the data center, and will be done in advance by administrators.
 * The root key will both be split into 3 pieces, in a Shamir arrangement
   requiring 2 pieces for recovery. Each of the pieces will be written to
   separate backup media.
 * The media may then be stored in a secure location (e.g. a safe).
 * The recovery keys will be generated on dedicated devices held by trusted
   individuals.
 * CNs will split their symmetric disk encryption keys into pieces and ECDH box
   them to each of the recovery public keys.
 * During recovery, a challenge-response procedure will be used to contact the
   trusted individuals and their hardware tokens and collect N/M responses to
   reconstruct the key.
 * The root public key and initial headnode trust chain (including the first
   recovery configuration entry) can be written to the boot USB flash media for
   the initial headnodes, and transported to the datacenter as part of the
   deployment process.
    - As an alternative, the headnode setup process will accept the public key
      and trust chain root on the console.

This scheme will be implemented using a set of tools that can run on at least
OSX, Linux or SmartOS, to correctly generate the root and recovery keys and back
them up, and then also to perform restoration operations in an emergency.
Backing up credentials as part of generating them will not be optional, and the
tools will require backup media to be present to perform any operations, to
prevent administrator error.

A recommended outline of the full deployment procedure is included in the
sections <<green-field>> and <<brown-field>>, which include examples for both
a "small setup" deployment not using a pre-flight environment, and a larger
deployment using one.

The following table highlights the recommended options for long-term key backup,
as well as a recommended verification and refresh interval for each.

The verification interval indicates how often (at a minimum) an administrator
should inspect and verify the data on the backup media to check its integrity.
The refresh interval indicates a minimum interval at which administators should
expect to have to copy the data to fresh media. Even if the current media
passes inspection, it is recommended that media older than this still be
replaced.

.Backup media recommendations
[options="header"]
|===

| Media type               | Verification interval | Refresh interval

| Magnetic tape (LTO, DAT) | 5 years               | 10 years

| Printed archival paper   | 3 years               | 10 years

| Optical (CD, DVD, BD)    | 1 year                | 5 years

| Flash (SD, CF)           | 1 year                | 3 years

|===

### Use of zone keys and certificates by customers

Quite aside from the internal use of zone keys and certificates within Triton's
components, they are also expected to be used by customers.

In conjunction with the RBACv2 work (RFD 48), signing requests to Triton
services (such as CloudAPI) using a zone authentication key will grant
authentication as a "machine principal". This principal may be added to roles by
a customer, in order to grant it authorization to manage resources under the
account.

The `keyId` string used is expected to include the full UUID of the zone in
question, and the UUID of the CN which hosts it. This mechanism will not
require the use of the zone certificate.

Since the existing `triton` tools and libraries already support the use of the
SSH agent for key storage, it is expected that they can be used with the
zone soft token without significant modification (they may require some in
order to generate the `keyId` correctly, but this is as yet unclear).

The existing support for account-key-signed certificates for Docker and CMON
will be extended to support the use of those interfaces as a machine principal,
as well. This mechanism is preferred for customer end-use here rather than the
TLS certificate signing key, as it matches the interface already used elsewhere,
reducing the amount of code needed to be specific to machine authentication.

Though it is somewhat out of scope here, it is expected that mechanisms for
grouping machines as access control targets (e.g. RFD 48 style projects) may
also be useful for grouping machines as principals. In this way it should be
possible to grant some group of machines access to account resources and have
this apply to newly provisioned members of that group automatically.

While zone SSH certificates and certificates signed by the TLS certificate
signing key are not used for Triton authentication, endpoints on CloudAPI will
be added to assist in the validation of zone certificates by customer code or
services. These include fetching the current full set of headnode CA
certificates for the X.509 chain. This should allow zone keys and certificates
to be used for other purposes as well (such as bootstrapping a chain of trust
for customer systems).

In particular, it is expected that full support for this mechanism will be
developed to assist with the bringup of the Hashicorp Vault product. Vault
should hopefully also be able to take advantage of the Soft HSM key system.

## kbmd

### Introduction

kbmd (read: kaboom-dee) has 3 big areas of responsibility:

Firstly, it's responsible for the "recovery" process -- when a server
has lost its primary Yubikey/PIV token, it is responsible for providing
the interface an administrator uses (either on the console or a pty) to
recover encryption keys, set up a new Yubikey, and get the system back
on track. Since this logically requires it to be able to set up new
Yubikeys from scratch, it's also involved in the initial setup process
to keep all the responsibility for that together.

Secondly, it's responsible for the "unlock" process at boot --
determining whether the primary Yubikey is available, getting the PIN
(from boot-time module or pool config for standalone, or spawning a
client to talk to CNAPI or whatever headnode service stores it for a
CN), and if those fail, deciding whether to enter "recovery".

Thirdly, it's responsible for everything during normal runtime that's
required to make those two processes work. This mostly means keeping
track of the encrypted data boxes on the machine and the "recovery
registry" (getting to that in a sec). It also means operating a door
server and accepting requests from a commandline admin tool, "kbmadm".

The name "kbmd" reflects this -- "Key Backup and Management Daemon".
(Definitely not a backronym so we can pronounce it "kaboom". Definitely
not.)

Encrypted boxes on the system fundamentally come in two forms -- there's
the boxes associated with the zpool (one set for the primary Yubikey and
one set for recovery), and then there are boxes for each of the keys
stored by the RFD77 soft-token (recall that the soft-token individually
encrypts its keys even when zpool encryption is enabled, as part of the
effort to make a "class break" that compromises all of the keys on the
system in one single operation, as difficult as possible).

The boxes themselves are stored as a zfs property (`rfd77:config`).  The
currently size limitations of zfs properties should allow for a single
property to store approximately 8 boxes worth of data.

The soft-token keys have to be boxed individually to the primary token
(so that the primary token can't unlock all of them in a single
operation), but they do not have to be boxed individually to the backup
keys. In fact, it would be pretty inconvenient if they were, because we
would have to do the challenge-response process at least N times for a
machine with N zones on it.

So instead, the soft-token keys' backup comes in the form of a single
large box (keyed only to the backup keys) which unlocks all of them.
Every time we need to add or remove something from that box, we have to
regenerate it from scratch using the individual boxes targetted to the
primary Yubikey. So we keep a plaintext record next to it of the
locations of all of the primary Yubikey boxes on disk. We call this
whole structure together the "recovery registry".

This implies that the storage of these keys is somewhat managed by the
system, and it is. When the soft-token wants to generate a new key, it
has to coordinate with kbmd (via its door) to let it know the correct
filesystem paths to find the primary boxes, and make sure the entries
are added to the recovery registry and everything there is dealt with.

Since this happens when a new zone is provisioned, and an attacker is
generally assumed to be able to provision things in the system, we don't
really want this to cause us to bring keys belonging to existing zones
into RAM in a predictable controllable fashion. So the recovery registry
is in fact split into two parts -- the "old generation" and "new
generation". When we add new keys we add them to the "new generation"
and regenerate that only. Then, every 6-12 hours or so (completely at
random) we combine the old and new generations together and regenerate
the whole thing. This avoids an attacker being able to control the
timing and nature of this operation easily (and it also means we don't
have to regenerate the whole registry every time we make a change -- we
basically bulk a bunch of changes up).

### In depth: recovery

A recovery instance is created when another program running as root with
full privs connects to the kbmd door and sends a "begin recovery"
request. If kbmd decides it needs to initiate recovery on the console
(e.g. during boot), it forks a child to start kbmadm to do this and
places it on the console.

The "begin recovery" request is followed by a "conversation" similar to
a PAM conversation: kbmd gives the client some text and instructions on
what to ask the user and what options to allow them to reply with, the
client replies with the user's response, kbmd gives more questions to
ask the user etc.

At the end of the conversation, kbmd does not reply to the final
response until recovery is complete.

kbmd then does the following:

1. A new token value is added to the `rfd77:config` zfs property on the
   primary zpool (i.e. zones).
2. New managed box files with the GUID of the new token are created.
3. Notify everybody of the change (presumably using the gossip protocol?)
4. Remove the old primary token from the `rfd77:config` zfs property on the
   primary zpool.
5. Cleanup old managed box files: any box for a GUID not in `rfd77:config` or
   otherwise not known are deleted.

### In depth: first-time setup

The "setup" process is set out the same as recovery, except that in the
final stages, kbmd gives an nvlist of ZFS properties to set on the pool
root FS and the raw key material to supply to the stdin of `zpool
create`. The client is responsible then for creating the pool using this
information and replying again once that has been completed. The client
has to complete this step to avoid kbmd needing to handle the entire
pool creation process (including disklayout etc), since the key material
has to be given at creation and cannot be given later.

The ultimate command that get runs should look something similar to:

```
zpool create \
    -O encryption=on \
    -O keyformat=raw \
    -O keylocation=prompt \
    -O rfd77:config=xxxx \
    zones ....
```

Where the disk layout values are just passed through from the setup and we
write the key value out to stdin of the zpool process from kbmd.

Likely the joysetup.sh script will need to be updated to interact with kbmd
when encryption is desired while setting up a new CN.

### In depth: events

The kbmd door also provides a mechanism for clients to receive events.
The principal kind of event that clients are interested in is when the
"primary token" of the system is about to change and has changed. All
persistent users of kbmd are expected to handle this event.

### In depth: managed boxes

A client of the kbmd door may send a command to create a managed box
(like the soft token key boxes). The command takes a "path pattern",
which looks like: "/zones/abcd123/keys/auth.%s"

If the current primary token has GUID 995E171383029CDA0D9CDBDBAD580813,
the client must have already created
"/zones/abcd123/keys/auth.995E171383029CDA0D9CDBDBAD580813" as a PIV-box
format file, a single box keyed to that current primary token.

kbmd will open that box and set up entries in the backup registry before
returning from the door call.

Thereafter, the application may not delete or modify the
"/zones/abcd123/keys/auth.*" files, but it may open them for reading in
order to retrieve the data held within. It must be subscribed to
notifications about a change in primary token so that it always opens
the correct auth.* file for the GUID of the current primary token at the
moment when it opens it (and if it talks to piv-agent and discovers the
key it needs to open a box is missing because of a primary token change,
it should wait for the change notification and try again).

If the system goes through recovery and has a new primary token, kbmd
will create a new primary token box file with the new GUID based on the
backup registry and the application will find it by asking for the new GUID.

### Interaction: piv-agent

The piv-agent is the intermediary that most other processes on the
system will go through to make use of the Yubikey. piv-agent will
connect to the kbmd door at startup and ask for the primary token and
PIN, as well as setting up a subscription for primary token change events.

If it receives a notification about the primary token changing, it
changes its own configuration to use that new token (and new PIN) for
all subsequent requests that it handles.

### Interaction: soft token

Like the piv-agent, the soft token daemon is interested in the primary
token changing -- both for signing certificates and for making use of
managed boxes.

### Interaction: ZFS

While kbmd does not involve itself directly in the creation of a new
encrypted pool (the setup process is expected to use the data it
provides to do that), after setup has completed it will manage the
"rfd77:config" ZFS property on the pool itself directly. This will be
read during boot and written during recovery or rekeying. kbmd itself
will also make the libzfs call to provide key material to the pool
during the unlock process (since it needs to do this and mount at least
some of the pool's filesystems before it can check on the state of any
managed boxes on disk).

### Interaction: Triton

:OS-7183: https://smartos.org/bugview/OS-7183

kbmd will have a dependency on an SMF service that brings up the "admin"
network early in boot ONLY when we booted with a networking.json (the
SMF dep is still there in non-Triton but the service is a no-op). This
is in order to enable it to retrieve the PIN for the primary token from
CNAPI after authenticating with its 9E key.  This is currently tracked as
{OS-7183}[OS-7183].

### Interaction: ccid driver

kbmd will utilize a new OS driver (ccid) to communicate with the PIV tokens.
This will be via the apis provided by the libchipcard library detailed in
<<os-infra>>.

## Relationship with TLS

To fully protect the Triton admin VLAN against IP and MAC spoofing attacks from
rogue network hardware, it will be necessary to begin protecting all connections
with TLS. Part of establishing a TLS connection is verifying the identity of
at least one party to the connection, using X.509 certificates.

Note that while TLS server authentication is expected to always be in use, the
providing and verifying of client certificates will be limited to those cases
where HTTP signature authentication cannot be reasonably used.

The zone TLS certificate signing key is set aside for the purpose of producing
TLS credentials. Core services will generate local keys (which may be rotated)
for use by TLS servers, protected at rest by the Soft HSM key. A signed
certificate and chain will be obtained through the soft token interface to allow
these to be validated to others.

It is the responsibility of any Triton service to ensure that it obtains a
new certificate chain for its TLS server endpoints before the expiry of a
previous chain.

As these certificates have an enforced short lifetime of 60 seconds, no
specific provision for certificate revocation is needed: only a requirement that
the list of valid CA certificates be kept up to date by clients to match the
output of the headnode gossip system.

## Service startup step-by-step

### CloudAPI

 . The Trusted CN hosting the CloudAPI instance boots up (see <<cn-boot>>
   for more details)
 .. It starts up the zone soft token manager daemon, which will LoFS mount
    sockets into all zones (see <<soft-token>>). The daemon does not unlock the
    keystores at startup.
 . The CloudAPI zone begins to start up
 .. Soft token socket is mounted into the zone.
 . SMF service `cloudapi` starts -- it execs `node`
 . CloudAPI calls into the `triton-registrar` library to set up its service
   registration
 .. Registrar opens the soft token socket and retrieves the public key and
    certificate signed by the GZ.
 ... Soft token manager daemon accepts the connection on the socket in the zone
     and forks off a dedicated privilege-separated child for this zone. The
     child then decrypts the keystore and loads it into memory.
 .. Registrar connects to binder zones and begins registration by writing a
    signed statement about the CloudAPI zone's IP address and keys, including
    the SSH certificate signed by its CN.
 .. Binder receives and validates the registration
 ... First, binder retrieves the list of valid Trusted CNs from the gossip service
     on its host CN (via the soft token socket)
 ... Then, it compares the signature on the certificate given by the registrant
     to this list and finds it was signed by a valid Trusted CN
 ... The certificate presented includes metadata about the zone, including any
     values of `sdc_role` or `manta_role` tags. Binder validates that such
     values should be allowed to register under the given DNS name.
 ... After validating the signature on the statement from the registrant, binder
     begins serving DNS records about it.
 . CloudAPI opens its cueball pool to connect to VMAPI
 .. Cueball is running in bootstrap mode, and first establishes a bootstrap
    resolver to connect to binder
 ... The bootstrap requests each binder's certificate by looking up the binder
     service hostname with rrtype CERT (see RFC4398)
 ... The bootstrap resolver then retrieves the list of valid Trusted CNs from the
     gossip service on its host CN, and uses this list to validate the binder
     instances' certificates. It also checks that the `sdc_role`/`manta_role`
     value matches up.
 ... The TSIG information on the response is also validated.
 ... The bootstrap emits only the binders that pass validation (along with their
     keys) to be used as resolvers.
 .. Cueball begins service resolution for VMAPI
 ... It uses the resolvers from the bootstrap stage to contact binder and
     request SRV records for VMAPI (and validates the response's TSIG using the
     keys from the bootstrap).
 ... Validated records are emitted as backends
 .. Cueball connects to VMAPI
 ... TLS is established, and the VMAPI's certificate and chain is validated
     against the known CA certificates (obtained by querying the soft token).
 . Now CloudAPI is registered and connected to VMAPI. It repeats these steps
   (without bootstrap, since that's already done) for other services.
 . When CloudAPI wants to make a request to VMAPI, it takes a pre-validated
   TLS connection from the pool and makes an HTTP request on it.
 .. The outgoing HTTP request is signed with the zone key of CloudAPI, and
    includes CloudAPI's registered binder hostname (the service name) as part
    of the keyId.
 .. VMAPI requests the CERT records associated with the name connecting to it
    from binder and validates that a key there matches the one signing the
    incoming request.
 .. Then, VMAPI validates the connecting service name against its own policy of
    which services are allowed to talk to it, and decides whether to accept or
    reject the request.

[[cn-boot]]
## CN boot

Unlike headnodes, ordinary Triton CNs boot over the network. Today, this is
designed to happen by launching the iPXE binary from flash media within each
server. The iPXE binary then makes a DHCP request, and receives a response
containing an HTTP URI from which to fetch the kernel and `boot_archive`.

iPXE supports HTTPS with certificate validation, and this will be used to secure
the CN boot process. It is currently considered unreasonable to add a full
software stack needed to produce signatures from the Yubikey's asymmetric keys
in iPXE, however, so it is proposed that anonymous access to the kernel image
and `boot_archive` be maintained as it is today (i.e., the authentication
at this stage will be one-way: the CN verifying the boot server's identity,
guarding against rogue DHCP and HTTP servers).

Since iPXE's certificate validation mechanism is limited to a set of CA
certificates, which have to reside on the same flash media as iPXE itself, we
treat boot-up here slightly differently to regular service-to-service (or
CN-to-service) authentication.

On the flash media with iPXE will be a set of self-signed X.509 certificates
describing the keys of each of the headnodes in the datacenter at the time when
the flash media is prepared.

The `booter` zones in the installation will generate a local TLS private key
each, and have it cross-signed by the signing keys of all the headnodes in the
data center. They will serve the full set of cross-signed certs in their TLS
handshake, as alternative chains footnoteref:[alt-chains,"Alternative chains"
here refers to the TLS notion of providing a single entity certificate, signed
by a single issuer DN, and then providing multiple certificates for that issuer
DN that are signed by different upstream issuers themselves. This practice is
already commonly used in the Internet today when introducing new CAs and is
quite widely supported.], so that the flash media need only contain one
headnode in common with the real current set for the boot to be successful.

Once a CN has been set up and is operating normally, it will periodically
mount its boot flash media and update the set of headnode CA certificates stored
there.

Some Triton installations do not boot iPXE from flash media, and instead use the
built-in PXE ROM in their system. Unfortunately, the only known way to build an
authenticated system around the firmware PXE is to leverage the EFI Secure Boot
and TPM features of a modern system, and support for using these with PXE is
difficult (due to lack of general EFI support) and somewhat inconsistent between
server vendors. It would also require the ability to modify at runtime the
certificates stored in firmware for boot signing, which currently is not a
well-supported procedure, regularly subject to vendor firmware bugs and
exclusion.

For this reason, installations which depend on system PXE firmware will not have
a fully secured boot procedure, and will not meet all of the stated goals of the
system. This may be revisited at a later date.

### EFI Secure Boot

No provision is made in this document for the implementation or management of
EFI Secure Boot in Triton. EFI support in illumos is not yet complete, and
several unresolved problems remain before a design can be proposed here.

This will likely be the subject of a future RFD.

[[green-field]]
## Green-field deployment step-by-step

### Installation

This setup process will need to provision a KBMAPI instance and setup the
head node token (probably more as well).  If the head node is to be encrypted,
then it must be setup (token setup, encryption enabled) at the time of zpool
creation.

NOTE: This section needs updating after the change to personal recovery keys
and updates on the trust chain.

This section will run through the full set of steps needed to deploy Triton
with full RFD 77 security enabled.

We begin the process by setting up the root key on an administrator workstation.
On this workstation, we will begin by burning 3 DVD-Rs on which to store key
backups.

After inserting the first blank DVD-R:

[source,shell]
----
alex@mbp:~$ triton-keymaster init-media dvd <1>
Found blank DVD media in HL-DT-ST DVDRW GX30N RP09 (scsi 1,0,0) <2>
Initialize? [Y/n]
Generating media key... done
Writing session... 10% 25% 50% 75% 100% done
Short name to refer to this media? [214cc7d2] sfo-001 <3>
----
<1> We want to initialize a new DVD type backup media. The name we give here
    refers to the storage plugin to be used.
<2> The plugin detects that we have a blank unused DVD-R in one of our drives.
<3> This name will be used with later `triton-keymaster` commands. If we want
    to use this same media from a different machine, we can copy the file
    `~/.triton/keymaster.json` or use `triton-keymaster add-media` and
    the full media identity string.

We perform these same steps for the subsequent 2 DVD-Rs, naming them `ord-001`
and `nyc-001`.

[source,shell]
----
alex@mbp:~$ triton-keymaster init-media dvd -y -n ord-001 <1>
Found blank DVD media in HL-DT-ST DVDRW GX30N RP09 (scsi 1,0,0)
Generating media key... done
Writing session... 10% 25% 50% 75% 100% done
alex@mbp:~$ triton-keymaster init-media dvd -y -n nyc-001
Found blank DVD media in HL-DT-ST DVDRW GX30N RP09 (scsi 1,0,0)
Generating media key... done
Writing session... 10% 25% 50% 75% 100% done
----
<1> `-y` means "don't prompt me for confirmation", and `-n` is used to give the
    media short name.

Now we generate the root keys for the datacenter:

[source,shell]
----
alex@mbp:~$ triton-keymaster init-dc us-west-1 -m sfo-001,ord-001,nyc-001 <1>
Number of backup media required to recover root key? [2] <2>
Generating root key... done
Generating ZFS recovery keys... done
Ready to write piece for backup media sfo-001.
Attach where? [LOCAL/remote/file] <3>
Found sfo-001 in HL-DT-ST DVDRW GX30N RP09 (scsi 1,0,0)
Writing session... 10% 25% 50% 75% 100% done
Ready to write piece for backup media ord-001.
Attach where? [LOCAL/remote/file]
Found ord-001 in HL-DT-ST DVDRW GX30N RP09 (scsi 1,0,0)
Writing session... 10% 25% 50% 75% 100% done
Ready to write piece for backup media nyc-001.
Attach where? [LOCAL/remote/file]
Found nyc-001 in HL-DT-ST DVDRW GX30N RP09 (scsi 1,0,0)
Writing session... 10% 25% 50% 75% 100% done
----
<1> The `-m` option allows you to supply the names of the backup media keys to
    use for this datacenter. If not supplied, you will be prompted.
<2> These answers can also be supplied as commandline arguments.
<3> After the initial media setup, backup media can be accessed in multiple
    different ways by the `keymaster` tool. They can be attached locally to
    the machine it is being run on (as shown here), or attached to a remote
    machine (with `keymaster` also installed), or written to a file to be
    transferred later. The key backups are encrypted in transit and cannot be
    read without the backup media itself.

At this point, we can also write the recovery keys to some hardware tokens to
place in storage with the backup media. This is optional, but recommended for
production deployments: if an administrator has to step in to recover a CN from
a broken hardware token late at night (with possibly impaired judgement), it is
better to handle the keys on a secured device like a USB token where it is
harder to make mistakes that may compromise the key itself.

[source,shell]
----
alex@mbp:~$ triton-keymaster write-token us-west-1 <1>
Which ZFS recovery key to write? [A/b/c] a <2>
Need to read key pieces from 2 more backup media.
Attach where? [LOCAL/remote/file]
Found sfo-001 in HL-DT-ST DVDRW GX30N RP09 (scsi 1,0,0)
Reading data... done
Need to read key pieces from 2 more backup media.
Attach where? [LOCAL/remote/file] remote <3>
Generating ephemeral key for remote challenge-response... done
Challenge: AavNCXVzLXdlc3QtMRAHb3JkLTAwMQdueWMtMDAxBWVjZHNhQQRKMlDjH/3I/x5JZzh3RqtoendWyr9Aj2hz4vV9lETQWdrxkmnbDeoMjRi9ll3mDALaP5tmkh4QIClvjjIJv0pOcS6Agg==
Enter this challenge at the prompt presented by `triton-keymaster respond' on the remote machine.
Then enter the response from the remote machine here.
Response: gavNBWVjZHNhEWNoYWNoYTIwLXBvbHkxMzA1DOsc+I31pxTqOL75flqSq5Cuz9hqfvKaRZHe8aEYkaMUBQZLbKyqunZRqiSHWsA0Dxo1HsVfBbIetNOqP2e5+JUnk9wS72B4sWmaojxC2nTUm6BiC+zAzW9px6uzwow5Y5KUFsYUHlSLB+mB
Found response from backup media ord-001.
All key pieces found.
Ready for Yubikey or Token for writing recovery key... ok
Found Yubikey (Yubikey 4 OTP+CCID), serial 4a701a, v4.3.1
Writing keys to Yubikey... done
----
<1> We have to specify the datacenter in order to fetch the backup media and
    key configuration.
<2> We can choose which of the 3 recovery keys to write out, so that we still
    enforce the same 2/3 rule for access.
<3> Here we choose to get a piece of the key from a remote system. This prints
    out a base64-encoded "challenge" value, which an administrator at the remote
    site can copy-paste into their "triton-keymaster" tool to generate a
    response.

The challenge-response cycle here is secure (encrypted) and unreplayable. The
use of the `respond` command on the remote administrator's machine looks like
this:

[source,shell]
----
john@mbp2:~$ triton-keymaster respond
Enter challenge: AavNCXVzLXdlc3QtMRAHb3JkLTAwMQdueWMtMDAxBWVjZHNhQQRKMlDjH/3I/x5JZzh3RqtoendWyr9Aj2hz4vV9lETQWdrxkmnbDeoMjRi9ll3mDALaP5tmkh4QIClvjjIJv0pOcS6Agg==

Challenge purpose: for master key recovery from backup media.

This is NOT a challenge used to recover a compute node with a broken Yubikey.

Datacenter: us-west-1
Key being recovered: ZFS recovery key A
Backup media they have: sfo-001
Backup media they want from you: ord-001, nyc-001

Challenge was generated 3 minutes ago by user "alex" on host "mbp"

WARNING: Responding to this challenge will give the remote party an entire ZFS
         recovery key. If they possess 2 of the set of 3, they will have enough
         information to decrypt the disk of ANY node in datacenter "us-west-1".
Respond to challenge? [y/N] y

Need to read key pieces from backup media: ord-001, nyc-001.
Attach where? [LOCAL/remote/file]
Found ord-001 in HL-DT-ST DVDRW GX30N RP09 (scsi 1,0,0)
Reading data... done
Response: gavNBWVjZHNhEWNoYWNoYTIwLXBvbHkxMzA1DOsc+I31pxTqOL75flqSq5Cuz9hqfvKaRZHe8aEYkaMUBQZLbKyqunZRqiSHWsA0Dxo1HsVfBbIetNOqP2e5+JUnk9wS72B4sWmaojxC2nTUm6BiC+zAzW9px6uzwow5Y5KUFsYUHlSLB+mB
----

[[brown-field]]
## Brown-field deployment

 * Deploying this on an existing DC

:RFD67: https://github.com/joyent/rfd/tree/master/rfd/0067

For an existing DC, one current limitation is that it will be difficult to
encrypt the zpool of the headnode.  This is due to the requirement that
encryption is enabled at the time of zpool creation. When complete,
{RFD67}[RFD-67] might allow one to designate a new machine as a new encrypted
headnode, and migrate the services from an unencrypted headnode to an
encrypted headnode.

For reasons similar to the headnode, one cannot take a live running compute
node and encrypt it's zones pool.  Instead one must re-setup a CN (and
recreate the zones pool).  However, unlike with the head node, there is
the possibility of (capacity permitting) evacuating a compute node (migrating
all instances on a CN to other CNs), perform the re-setup, release the new
encrypted CNs for use, and then repeat as desired.

## Implementation and intermediate states

So far, we have described the eventual state of affairs that Triton will be in
after a full implementation of this document. However, the process of
implementation will necessarily involve some intermediate states of development,
which will likely also be deployed to some installations along the way.

Additionally, not all administrators of Triton installations will see fit to
deploy with hardware tokens -- and it may be prohibitively difficult to do so in
some cases -- e.g. deployments within virtual machines for development.

 * Do the USB key and token support stuff first
 * Then soft-token (well, at the same time really)

 * The road to validating everything in the admin vlan, what intermediate states
   will look like while upgrading.
 * What things will look like if you never add any Yubikeys (TLS with just
   self-signed certs, open trust).

## PostgreSQL and Moray

 * Auth and TLS. Using LDAP to validate signatures as passwords?
 * In current version of PostgreSQL, the main limitation for using mTLS for
   AuthN/AuthZ is that PG has not supported reloading of certificates without
   a server restart. PostgreSQL now has certificate reloading on master, not
   yet in PG9.6. Reload is triggered by SIGHUP and/or "pg_ctl reload."
   Backporting a patch to PG9.2 would not be difficult
   (https://github.com/postgres/postgres/commit/de41869b64d57160f58852eab20a27f248188135[postgres change on master].)

[[soft-token]]
## Zone soft token details

The soft token consists of a number of key components:

 * The ECDH private key, stored in the CN's hardware token
 * The soft token key data files, stored encrypted on ZFS within the zone's
   dataset
 * The SSH agent protocol socket, placed as a UNIX socket within the zone's
   filesystem
 * The soft token daemon itself, running within the global zone, and listening
   on the UNIX socket

### Soft token key data

Soft token key data will be stored in the `/zones/$uuid/softhsm` directory.
Each key stored on behalf of the zone will be stored in a separate file,
encrypted (and authenticated) using ChaCha20-Poly1305.

The file format will consist of an nvlist with the public key of the hardware
token, a DH box containing the symmetric key to decrypt the rest of the data, as
well as the MAC and details of the algorithms in use. The MAC will be
constructed to cover the algorithm metadata fields.

### SSH agent socket

The SSH agent socket for communicating with the soft token will be placed in
the `/.zonecontrol` directory.

The existing `metadata.sock` inside the `zonecontrol` directory currently relies
on the permissions of the enclosing directory to manage access to the metadata
socket. These permissions will be moved to the socket itself, and the
`/.zonecontrol` directory will be world-readable and world-traversable. The
agent socket will use privileges, not filesystem permissions, to manage access.

The socket file itself within `/.zonecontrol` will be named `token.sock` (i.e.
its full path will be `/.zonecontrol/token.sock`). The socket file will be
world-writable and world-readable.

Upon a connection being made by a client process, the soft token daemon will
examine the `cred_t` of the connecting process. Either a new system-wide
privilege bit, `PRIV_ZONE_TOKEN` will be added, or a parametrized privilege will
be implemented, and any connecting process in possession of this privilege will
be allowed to use the soft token.

This privilege will be part of the default zone-wide limit set, but not part of
`basic` or the ordinary user privilege sets. This means that by default, only
root will be able to use the soft token, but end-users can configure their zones
to give this privilege to ordinary users or single processes, and processes can
give up the ability to use the soft-token if they no longer require it (enabling
privilege separation models to be used).

### Soft token daemon

The soft token daemon is started in the global zone as a child of the soft token
manager process. The manager itself is started by SMF.

The top-level manager process' role is to manage the lifecycle of socket files
and lofs-mounting them into zones. Each time it creates a new socket for a
given zone, it forks into a child which handles that zone.

The zone child of the manager is a privileged process whose role centers around
management of key material. It maps dedicated areas of memory (with `MAP_SHARED`
supplied to `mmap()`) for the placement of keys, fills them with the encrypted
key data, and then forks.

This final child is the process which is responsible for speaking the SSH
agent protocol and performing cryptographic operations. It drops all privileges
(including those in the `basic` set) before accepting any connections. To unlock
keys, it sends a fixed-size request on a pipe back to the key manager process,
which decrypts the keys in-place in the shared memory segment.

#### Performance and accounting

Unlike a regular SSH agent, the soft token daemon final process (serving the
real workload of the zone) will be multi-threaded. Operations will be carried
out by worker threads in a thread pool of limited size. This enables both
pipelining of operations within a single agent connection, and also concurrency
across multiple connections.

Eventually, a mechanism will be used to place the final child process into the
non-global zone for CPU accounting purposes, without making it able to be
traced or debugged by the zone (this will be analogous to a system process in
the global zone).

#### Hardware memory protection

Pending hardware and operating system support, the soft token will support the
use of Intel SGX enclaves (and the analogous features on AMD platforms) to
protect the key data and operating state of the soft token in memory.

This will defend against a variety of attacks on the soft token from other parts
of the system, as well as cold-boot attacks on system memory. Noting that, as
the soft token is a signing oracle in regular operation anyway, the goal here
is to prevent bulk fast access by an attacker to all the keys on a machine (a
kind of "class break"), not absolute inviolability.

SGX has been the subject of much industry discussion in recent months, and the
results achieved by others with it have been mixed. However, as our goal here is
not to achieve an impregnable enclave within a totally untrusted operating
system, but instead to simply make sure that there is no method of obtaining
keys faster than to ask the hardware to decrypt all the key files on disk, we
should be well-placed to make use of it.

#### Cache side-channel mitigation

:uri-cat: http://palms.ee.princeton.edu/system/files/CATalyst_vfinal_correct.pdf

On modern Intel CPUs, the soft token will (pending OS support) make use of the
Intel CAT feature to mitigate CPU cache timing side-channel attacks. This will
be done along the lines of the {uri-cat}["CATalyst" paper] where a special
subset of the L3 cache capacity on the system is set aside for transient use in
cryptography, and dedicated pages for this purpose pinned into cache so they
cannot be flushed out (containing both the code and data used in the sensitive
operation).

At the same time, we plan to make use of improvements in hyperthread scheduling
to avoid sharing any L1 cache between soft-tokens or between soft-tokens and
customer workloads.

Soft-token processes also will not share any memory pages (including code pages)
with each other or any other part of the system -- this is aided by an operating
system facility to mark binaries and shared objects are "unshared" so that they
are always duplicated into each process that maps them. KSM (kernel same-page
merging) and other similar mechanisms are not (and will not be) supported by
illumos.

As well as this direct mitigation, the algorithms chosen (see the
<<crypto-algos>> section) for soft-token usage are chosen with side-channel
leak prevention in mind.

The chosen algorithms combined with these mitigation techniques should prevent
most known mechanisms of memory timing side-channel leakage from the
cryptographic algorithms run in the soft-token, including Flush+Reload and
other related attacks.

[[hardware]]
## Hardware implementation

Both the Yubikey and JavaCard USB tokens present a common interface -- the USB
CCID (Chip Card Interface Device) device class. As this (unlike the HID
interfaces on Yubikeys and other devices) is an open interface, with readily
available specifications, this is the interface that is used for the purposes
of this design.

The CCID interface was originally intended for communication between hosts and
smartcards that speak the ISO 7816-4 protocol stack. Even though the USB
devices discussed here are not a smartcard in a card reader, they present
themselves to the host as if they were one. This means that the ISO 7816-4
protocol must be used to communicate with them, just as for a real smartcard.

While the ISO 7816 family of specifications specifies the commands and protocol
used for this communication, as well as some aspects of the data model on
compliant cards, it does not fully specify the structure and organisation of
key material storage.

As a result, additional specifications have arisen to describe the "directory
structure" and missing details of data model for particular applications using
cryptographic smartcards. One of the most commonly known and implemented of
these is the NIST Personal Identity Verification (PIV) standard. This standard
is implemented by both Yubikeys and other JavaCard token manufacturers.

As a result, for asymmetric crypto operations, the interface that the RFD77
implementation uses is PIV over ISO 7816-4 over CCID over USB. We also use
this interface for performing ECDH to derive disk and soft-token storage keys.

PIV specifies a fixed number of key "slots" on the token, and rules about
whether PIN or biometric authentication, or a secure channel is required for
each. As we are not identifying human cardholders or using a non-contact
interface like NFC, we will mostly avoid using these features, with the
exception of the PIN which we will use to require the network connection to a
core service for a compute node to boot.

:uri-yubico-piv: https://developers.yubico.com/PIV/Introduction/Yubico_extensions.html

Yubico have implemented a number of {uri-yubico-piv}[extensions] to the PIV
specification which include support for importing a key generated off-card,
setting management keys, changing PIN usage policies and performing attestation.
We will not have a hard dependency on these extensions in the implementation of
this RFD, but we may implement optional support for using them.

[[os-infra]]
### Operating system infrastructure

Most other open-source operating systems (e.g. GNU/Linux distributions) use a
userland-only suite of software for interacting with CCID smartcards. These are
usually backed by `libusb` or similar (the leading example of such a suite
would probably be OpenSC and pcsclite).

Proprietary operating systems such as Microsoft Windows and the Apple Mac OS
have instead opted to implement fairly deeply integrated smartcard suites
in the operating system base, in order to fully support integration with other
operating system features (e.g. using smartcards seamlessly for user login,
or Windows domain machine authentication etc).

For SmartOS, we propose to implement a hybrid approach similar to the Apple
Mac OS. There will be a deeply integrated operating system component for card
identification and operational use, but card administration and deployment
operations will be handled by software running entirely in userland.

This will allow us to integrate deeply with operating system features such as
the fine-grained privilege model and RBAC, as well as zones. We will provide
a public interface specific to SmartOS (working title `libchipcard`), as well
as implementations of the PCSC API (compatible with `pcsclite` and Mac OS) and
a subset of PKCS#11.

Components built as part of this design (e.g. the soft token, and key provider
for ZFS) are expected to exclusively use the `libchipcard` interface, with the
exception of the deployment and administration tools, which will be largely
based on the PCSC interface (which will also make them largely cross-platform).

The OS infrastructure to be built out here, including the `libchipcard`
interface, will be the subject of a forthcoming RFD specific to their
implementation.

[[triton-infra]]
## Triton infrastructure

### KBMAPI

As mentioned in <<at-rest>>, a trusted node service will be needed in a
datacenter to store the pins and recovery data for all of the tokens in the
datacenter.  This service is the Key Backup and Management API (KBMAPI).

KBMAPI should be a fairly simple and minimal REST service.  API endpoints
(proposed endpoints are described in detail below, based on some early
prototypeing) provide the means for adding new tokens, removing tokens,
recovering tokens (i.e. replacing a token), as well as providing the PIN of a
token to an authenticated entity.

When a token is added, the KBMAPI service will need to generate a recovery
token (a random blob of data) that will be stored on the CN.  The recovery
token serves two purposes:  First, it is used by the CN as the recovery key
as described in <<prov-backups>>.  Second, it is also used by the CN as a
shared secret with KBMAPI for the purposes of replacing the token information
of a CN with the data from a new token.

Additionally, an operator may optionally decided to limit the tokens allowed
to be added to KBMAPI from a known set of tokens.  For Yubikeys, this would
involve specifying the serial numbers (or a range of serial numbers) of the
Yubikeys that are allowed to be added to KBMAPI.  Yubikeys include an
attestation cert that is burned in during manufacture that can be used to
validate that a request to add a token originates from a Yubico manufactured
device (essentially authenticating the serial number of the device).

#### Token object

The data managed by KBMAPI must be persistent (for hopefully obvious reasons).
Moray buckets are likely sufficient for this purpose.  The persistent token
data will likely resemble something similar to:

[json]
----
{
  "guid": "97496DD1C8F053DE7450CD854D9C95B4",
  "cn_uuid": "15966912-8fad-41cd-bd82-abe6468354b5",
  "pubkeys": {
   "9e": "ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYA...",
   "9a": "ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYA..."
  },
  "pin": "123456",
  "recovery_token": "jmzbhT2PXczgber9jyOSApRP337gkshM7EqK5gOhAcg=",
  "model": "Yubico Yubikey 4",
  "serial": 5213681,
  "attestation": true
}
----

All of the items above except the `recovery_token` field are supplied from
the token.  The `recovery_token` field is generated by KBMAPI during as a
part of the CreateToken API call as described below.

To protect the token data in Moray, we will rely on the headnode disk
encryption.

XXX: Even though the HN token will not use the GetTokenPin
API call to obtain it's pin, should we still go ahead and store the data for
the HN token in KBMAPI?

It's important to reiterate that complete removal of an token entry in KBMAPI
renders all data protected by that token inaccessible.  Regardless of the
form of the final endpoints used for KBAPI (a proposed set of of REST endpoints
follows this section), care must be taken to prevent accidential loss of the
token data due to operator error.

### KBMAPI Endpoints

These are the proposed endpoints to meet the above requrements.  They largely
document the behavior of the existing KBMAPI prototype (though in a few places
describe intended behavior not yet present in the prototype).

#### CreateToken (POST /pivtokens)

Add a new PIV token.

Required fields:
 * cn_uuid - The UUID of the CN where this token resides.
 * guid - The GUID of the token itself
 * pin - The pin for this token generated by the CN during the setup process.
 * pubkeys - The certificates created on the Token.  This should include at
least the 9E key (probably others).  These are the public portion of a
public/private key pair (and never contain any private keys of a token).
 * serial - String describing the serial# of the token

Optional fields:
 * model - String describing the model of the token

If a token with the same GUID already exists, an error is returned (there
are no specific requirements for what this error looks like -- whatever we
do in other services should be fine).

If the CN indicated by the `cn_uuid` field already has an assigned token,
the request should fail.  We might want to alter this requirement slightly
however as part of preserving a limited amount of history (see the DeleteToken
and Recovery sections below), and perhaps only allow one 'active' token for
a given `cn_uuid`, and reject the request if there's already an active token
assigned to the CN.

If the operator has indicated that only 'allowed' tokens can be added, the
serial number must be compared to the list of allowed tokens.  Additionally,
the serial number itself must be validated.  This likely will involve the
inclusion of additional fields in the request (details TBD).

If this is not a duplicate key, if the token is allowed to be added,
and if the token is not being assigned to a CN with an existing token assigned,
the recovery token is generated.  The recovery token is just a random blob of
data that is used as an encryption key.  As such, the method of random
generation should be suitable for cryptographic purposes.  The generated
recovery token value is merged with the JSON values supplied with the request
and saved in a moray bucket.

Aside from any standard pieces (HTTP result codes, status string, etc.), the
response minimally needs to include the recovery token.  Other data (such
as values supplied in the request) may be included if we think it may be
useful (e.g. to assist with troubleshooting).

[json]
----
{
    ... normal response code stuff ...
    "recovery_token": "xxxxx"
}
----

Since the recovery token is just a random binary blob, we will likely want
to encode the value of the `recovery_token` field (i.e. base64 encoding is
almost certainly fine for this).

#### ListTokens (GET /pivtokens)

Gets all the known pivtokens.  The main requirement here is no sensitive
information of a token is returned in the output.  With the above proposed
model, the sensitive fields would include at least the `pin` and
`recovery_token` fields.  It is possible additional fields might be added to
the list of sensitive fields in the future.

Aside from the above requirements, this can act like similar 'get items' type
requests in other services.  It seems reasonable to support filtering by
public attributes (e.g. ?cn_uuid=XXXX) as well as having support for windowing
of the results.

As the results of this request could be multiple tokens, the possible output
might look similar to:

[json]
----
{
    [
        {
            "guid": "12345...",
             ....
        },
        {
            "guid": "abcdef...",
            ....
        },
        ....
    ]
}
----

#### GetToken (GET /pivtokens/:guid)

Gets the public info for a specific token.  Output is similar to that of
ListTokens, except a single JSON object is returned (and again only public
fields are contained in the response).

#### GetTokenPin (GET /pivtokens/:guid/pin)

Like GetToken, except it also includes the `pin`.  The `recovery_token` field
is *not* returned.  This request must be authenticated using the 9E key of the
token specified by `:guid` to be successful.  The 9E key is just a certificate,
and should work similar to how we use SSH public keys to authenticate users
elsewhere in Triton API calls.  Unauthenticated requests should return
failure.

This call is used by the CN during boot to enable it to unlock the other
keys on the token.

#### DeleteToken (DELETE /pivtokens/:guid)

Deletes information about a pivtoken.  We should almost certainly require
this come sort of additional confirmation or authorization before this succeeds.

#### Recovery

Currently, the endpoints necessary for recovery are less well defined (as in
there aren't any proposed designs currently).  In a recovery situation, the
original token for a CN is unavailable due to damage, loss, etc. and must be
replaced with a new token.  Since the original token is unavailable to
authenticate the CN (as well as to load the zpool key), the recovery token
generated during the original CreateToken call (and securly saved on the CN) is
used to authenticate the recovery process.  An operator will direct kbmd to
perform a recovery.  This involves a series of challenge and responses, the
end result being the recovery token generated in the CreateToken call is
decrypted and able to be used to authenticate a KBMAPI request to replace the
missing token entry with a new one (such a request would also include all the
information of the new token).

An open question is what should this API call look like? PUT /pivtokens/:guid
is a possibility, or perhaps POST /pivtokens with a parameter
(maybe `?replace=guid` ?).

Any solution is going to require a way to identify the old token, the
recovery token value of the old token (to verify the request), and the
information about the replacement token (this information should be the same
information provided during token creation).  A new recovery token will need
to be created for the new token

To identify the token, it seems that the easiest way would be to include the
GUID of the old token in the recovery request.  This should be available via
the `rfd77:config` zfs property, but could also be queried by doing a
ListTokens request filtering on the `cn_uuid` field.

One idea for the form of the recovery quest could be:

[json]
----
{
    "old_guid": "xxxx",
    "recovery_token": "yyyyyy",
    "new_token": {
        "guid": ....
        <same data as CreateToken>
    }
}
----

A nice feature of organizing the recovery request in this manner is that it is
clear which data refers to the old token and which data refers to the new
token.

More likely, we'll want a way to keep the old token data for a limited,
possibly configurable (likely through SAPI) amount of time (such as 30 days)
as insurance against operator error.  We want only one token 'active' on
a CN at a time though

### Chassis Swaps

While not technically a situation that requires recovery, it is worth discussing
chassis swaps.  When a chassis swap occurs, the drives from system are
transferred to another system.  Triton then sees this as a new CN (as they
almost always have a different UUID), and updates it's various datastores
with the data from the CN.

This presents some potential complications when tokens are present that need
to be considered.  When a chassis swap happens, the KBMAPI data must also
be updated as a result.  There appear to be two potential ways of doing this:

1. Move the token along with the drives.  This is likely the simplest
approach, as all that should need to occur is the `cn_uuid` attribute of the
token data in Moray is updated with the new value.   However, care must be
taken to ensure that no token data is lost as a result of this (losing both
the pin for the token used to encrypt a pool and the recovery pool will
result in unavoidable data loss).  We will need to think and identify what
safeguards are necessary to prevent this from happening.  For example, what
happens if there's already a token associated with the CN?  Automatically
deleting the conflicting entry is unlikely to be desirable.

2. If the new chassis has it's own token, a recovery could be initiated that
replaced the token from the old chassis with the token on a new chassis.
This is likely to take more time than merely moving the token, which may
lead us to prefer option 1.  Though in this case we still would want to
think of appropriate safeguards to prevent loss of the token data in KBMAPI.

### Triton install

Since our model requires that encryption is setup at the time of zpool creation,
we must add support when installing head nodes and compute nodes to setup
encryption.  As stated in <<customer-features>>, we should not require
encryption to be an all-or nothing deal -- it should be permissible to allow
both encrypted and unencrypted CNs in a Triton install.

Once the CN authentication pieces are available, it may even be permissible
that a user to have no encrypted CNs, and instead uses the RFD77 delivered
pieces (with tokens on CNs) for encrypted/authenticated control plane
traffic between CNs and HNs. Either use will require the KBMAPI service to be
installed and running.

[[crypto-algos]]
## Cryptographic algorithms

One important part of any design involving crytographic primitives is the choice
of algorithms in use. This section is devoted to discussion about options and
trade-offs made in algorithm choice above.

### At-rest encryption

The algorithm to be used for at-rest encryption key derivation is ECDH on P-256
with KDF SHA2-512. This is chosen because:

 * Using ECDH with an ephemeral key to derive symmetric keys for authenticated
   file encryption is very well-studied and specified (e.g. as ECIES in SEC-1).
 * The ability to stack the encryption "boxes" to allow multiple EC private keys
   to be used to decrypt the final key has many desirable operational properties
   over a scheme based on symmetric keys (e.g. no need for online backups).
 * The P-256 curve is believed to be 128-bit secure and ECDH with it is well
   supported on both Yubikeys and JavaCard hardware.
 * Ed25519 and Curve25519 ECDH were also considered, but lack of hardware
   support makes them impractical at the present time.

### Public-private encryption

The algorithm used for hardware authentication keys is RSA at 2048-bit key
lengths. This is chosen because:

 * RSA is a widely used and well-studied cryptographic algorithm for signing
   and authentication.
 * The 2048-bit key length is chosen as a trade-off between security level and
   performance -- Yubikeys and JavaCards are very slow at computing 4096-bit
   RSA signatures (on the order of hundreds of milliseconds).
 * Alternatives are not well-supported:
   - Ed25519 is not supported in either Yubikeys or JavaCard hardware.
   - ECDSA on NIST P-curves is supported by Yubikeys but not most JavaCard
     hardware options at this time.

RSA in Smartcard devices has a mixed history of side-channel attacks, but modern
hardware has extensive mitigations to lower their impact. The lack of widespread
support for alternatives at the present time is the main limiting factor here.

### Soft token

Soft tokens will support Ed25519 and RSA-4096 for public/private cryptography.
They will also support ChaCha20-Poly1305 for symmetric key operations (with the
key protected by the same ECDH box scheme as above).

Ed25519 and RSA-4096 are chosen because:

 * Ed25519's reference implementation is of excellent code quality and readily
   useable for the soft token.
 * Ed25519 is highly side-channel resistant, particularly to CPU cache timing
   side-channels. The soft token must run on the same hardware as customer
   workload, and possibly the workloads of other customers, meaning that
   resistance to side-channel attacks is paramount.
 * RSA is available in addition to Ed25519, as Ed25519 is not yet widely
   supported in TLS and X.509 certificates. The RSA key can only be used for
   signing X.509 certificates as outlined above, and not for general
   authentication.
 * ECDSA has a questionable history with respect to side-channel attacks,
   with many more successful attacks documented than on the other algorithms
   considered, so it was eliminated.

ChaCha20-Poly1305 is chosen because:

 * It is a strong AEAD cipher + MAC combination that has been quite well-studied
   despite being younger than AES.
 * Its implementation is simpler and built from the beginning to support
   authenticated operation, when compared with AES and other families.
 * It is explicitly designed for side-channel resistance. While AES could have
   been chosen, assuming that AES-NI or SSE3 are available, it is desirable to
   not have to require these CPU features for the system to operate safely.

