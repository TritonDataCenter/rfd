# Building and testing jupiter.example.com with Jenkins

The trivial jupiter.example.com application consists of a number of stateless containers (or containers whose state can be easily reconstructed) running as a tiered application of Nginx fronting a PHP/Apache web content templating system.

### Jenkins

The operators use Jenkins to automate the process of building new containers for each pull request and branch in the origin repo. This requires that Jenkins jobs need to be able to interact with Mariposa via the CLI (or API).

- Jenkins must be provisioned with an SSH key for Mariposa
- Jenkins must be provisioned with an SSH key for GitHub deploy keys
- There needs to be a way to get the key onto Jenkins in the first place.

Note that Jenkins will effectively own the entire account until we have RBAC v2. Once RBAC v2 is live, the Jenkin user will need the following permissions:
- create projects
- copy projects/manifests
- update project manifests
- trigger deploys of projects

The Jenkins user will not need the following permissions:
- launch containers
- destroy containers
- change account information
- create new users or roles

Because Jenkins is going to build containers locally (via Docker on KVM), operators provision the SSH keys with traditional configuration management (ex. Ansible) and bind-mount this into the Jenkins container running on KVM. Optionally, the operators can treat the KVM as a traditional VM and deploy Jenkins to it via configuration management.

### GitHub

The code is hosted in GitHub. Most everything here varies only in detail with GitLab or Bitbucket. Each project (ex. jupiter.example.com) has a GitHub repository. The GitHub repository is configured with a webhook that sends push events to Jenkins. These push events include information on what branch and commit was pushed.

### Jenkins jobs

Jenkins should be configured with two jobs, one for development and one for production. The separate production job allows for manual approval of the build while reducing risk of accidental deployment (see below).

### Jenkins development/test job

This Jenkins job is configured to receive GitHub webhook pushes. The Jenkins job should run build and deployment tasks defined in the code repository so that the process for deployment is owned by the team that owns the code.

The Jenkins job creates a Mariposa project for each GitHub branch. The Jenkins job clones a template manifest (from the repo) for each new branch. Each changeset received on a given branch will be deployed as a new version of that manifest.

The job task might look like the following:

- Run `docker-compose build` locally
- Tags containers with the `GIT_BRANCH` and `GIT_COMMIT`
- Containers to a registry (optionally a private registry)
- Render manifest template to manifest
- `mariposa push manifest`
- `mariposa deploy version`

Mariposa will then see a new version of the manifest and deploy those containers for testing.

> Open question: when do you shut down containers or delete a project associated with a branch? We can receive `delete` events from GitHub web hooks, but it’s not clear Jenkins jobs can do anything with them.


### Jenkins production job

Despite the automation for continuous build and deploy of the staging projects, the deploys to the live site require manual promotion. After reviewing the relevant staging site and any logs generated by Jenkins, the operators merge a PR to master, triggering a new build of master that is tagged with the date and time of the build (the builds collect resources beyond the scope of the git repo, so the git SHA is not a consistent identifier). The master branch is built and staged using the same logic as the development jobs above in a project named <repo-name>-branch-master.

After testing and previewing that final staging build, the operators can choose to manually deploy that build to the live site. This manual deployment is done via a separate production job. This Jenkins job is not configured to receive GitHub webhook pushes. Instead, it is a parameterized job which deploys the parameterized tag (with a default like “production”). The end-user tags the release in git with a version and the tag.

```
git tag v1.2.3
git tag production
git push --tags
```

The operator uses the Jenkins UI (or CLI) to start the deployment job with the particular tag needed (with the default being part of the Jenkins job definition). The job tags the correct version as “production” and renders the production manifest with the new version and pushes it to Mariposa for deployment. Note that Mariposa isn’t watching registry server; you need to explicitly update images in IMGAPI to get new images, so updating the Docker image tag in the registry is not enough to trigger a deployment.

Note: We should surface the content hashes in the UI so that it’s obvious if a deploy with different image ID but the same Docker tag has occurred.

### Secrets

There are a small number of sensitive details that need to be carefully handled, including the SSL cert and key for Nginx, possibly TLS client and server certs for Consul, and a license key for the CMS. The application operators expect Mariposa to offer features to simplify that process.

Secrets can be injected by Mariposa into containers via instance metadata. These can be accessed via ContainerPilot preStart or the application.

In hardened environments the instance metadata could be a one-time token for something like Vault. The scheduler would need to coordinate with w/ the external secrets management server though (maybe for a post-MVP).

Jenkins would generate secrets as part of the job for the dev environment, add those to the manifest’s metadata (this isn’t in the manifest file itself!). This allows the dev credentials be ephemeral for each deployment.

For production metadata, the operators can add those secrets to Mariposa out-of-band from the CI server. This way Jenkins never gets the production secrets. It only has the manifest not the metadata for production. (Although in practice in has the keys to the universe until RBAC v2.)

### Versioning of components

- The source code is versioned in the GitHub repo and tagged, including the manifest templates
- The container image is stored and tagged in a Docker registry
- The service or project manifest version details the infrastructure configuration, including not just the image and tag for the container, the open ports, RAM allocation/package size, and other details.
- Should metadata be versioned? Or is it better not to version this so that it’s easier to destroy metadata and rotate creds?
