# Building and testing jupiter.example.com with Jenkins

The trivial jupiter.example.com application consists of a number of stateless containers (or containers whose state can be easily reconstructed) running as a tiered application of Nginx fronting a PHP/Apache web content templating system.

### Jenkins

The operators use Jenkins to automate the process of building new containers for each pull request and branch in the origin repo. This requires that Jenkins jobs need to be able to interact with Mariposa via the CLI (or API).

- Jenkins must be provisioned with an SSH key for Mariposa
- Jenkins must be provisioned with an SSH key for GitHub deploy keys
- There needs to be a way to get the key onto Jenkins in the first place.

Note that Jenkins will effectively own the entire account until we have RBAC v2. Once RBAC v2 is live, the Jenkin user will need the following permissions:
- create projects
- copy projects/manifests
- update project manifests
- trigger deploys of projects

The Jenkins user will not need the following permissions:
- launch containers
- destroy containers
- change account information
- create new users or roles

Because Jenkins is going to build containers locally (via Docker on KVM), operators provision the SSH keys with traditional configuration management (ex. Ansible) and bind-mount this into the Jenkins container running on KVM. Optionally, the operators can treat the KVM as a traditional VM and deploy Jenkins to it via configuration management.

### GitHub

The code is hosted in GitHub. Most everything here varies only in detail with GitLab or Bitbucket. Each project (ex. jupiter.example.com) has a GitHub repository. The GitHub repository is configured with a webhook that sends push events to Jenkins. These push events include information on what branch and commit was pushed.

### Jenkins jobs

Jenkins should be configured with two jobs, one for development and one for production. The separate production job allows for manual approval of the build while reducing risk of accidental deployment (see below).

### Jenkins development/test job

This Jenkins job is configured to receive GitHub webhook pushes. The Jenkins job should run build and deployment tasks defined in the code repository so that the process for deployment is owned by the team that owns the code.

The Jenkins job creates a Mariposa project for each GitHub branch. The Jenkins job clones a template manifest (from the repo) for each new branch. Each changeset received on a given branch will be deployed as a new version of that manifest.

The job task might look like the following:

- Run `docker-compose build` locally
- Tags containers with the `GIT_BRANCH` and `GIT_COMMIT`
- Containers to a registry (optionally a private registry)
- Render manifest template to manifest
- `mariposa push manifest`
- `mariposa deploy version`

Mariposa will then see a new version of the manifest and deploy those containers for testing.

> Open question: when do you shut down containers or delete a project associated with a branch? We can receive `delete` events from GitHub web hooks, but it’s not clear Jenkins jobs can do anything with them.


### Jenkins production job

Despite the automation for continuous build and deploy of the staging projects, the deploys to the live site require manual promotion. After reviewing the relevant staging site and any logs generated by Jenkins, the operators merge a PR to master, triggering a new build of master that is tagged with the date and time of the build (the builds collect resources beyond the scope of the git repo, so the git SHA is not a consistent identifier). The master branch is built and staged using the same logic as the development jobs above in a project named <repo-name>-branch-master.

After testing and previewing that final staging build, the operators can choose to manually deploy that build to the live site. This manual deployment is done via a separate production job. This Jenkins job is not configured to receive GitHub webhook pushes. Instead, it is a parameterized job which deploys the production version.

### Versioning

The Docker build process does not produce reproducible builds (in the [formal sense](https://reproducible-builds.org/)). The source code for the Dockerfile may be and should be versioned in git, but the final image artifact is a result of this source code, any base images (which might not be strongly versioned), and any external packages. So although we'll want to include metadata about the source version in the image, the canonical version for purposes of Mariposa needs to be the content hash of the image as provided by the [Docker content addressable storage](https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/#/content-addressable-storage) feature.

Our Jenkins build can introduce metadata about the source version into the final image via the `docker build --build-arg` flag combined with an `ARG` and `LABEL` in the Dockerfile. For example the build might include `docker build --build-arg GIT_COMMIT=$(git rev-parse HEAD) --build-arg GIT_TAG=1.2.3 .` and a corresponding Dockerfile section:

```
 ARG GIT_COMMIT
 ARG GIT_TAG
 LABEL com.example.git-commit=$GIT_COMMIT \
       com.example.git-tag=$GIT_TAG
```

Because the image tag produced by `docker tag` can be migrated from image to image, Mariposa can present this as a convenience but a given version of a manifest is associated with the content version and not the image tag.

All this results in a number of versions we need to call out:

- "source version": the commit and/or tag in git. Mariposa is uninterested in this and surfaces it only as image metadata.
- "image version": the content ID of the image
- "project version": the specific version of a project manifest and its metadata, which may include multiple image versions identified in either the manifest or the metadata.


### Deployment of the production version

Returning to Jenkins, the operator uses the Jenkins UI (or CLI) to start the deployment job with the particular image version needed (with the default being part of the Jenkins job definition). The job tags the correct image version as "production". The job then creates a new project version (version of the Mariposa manifest along with metadata changes) that updates the image version to be used.

Mariposa then fetches this update from the registry server. Note that Mariposa isn’t watching registry server; you need to explicitly update images in IMGAPI to get new images, so updating the Docker image tag in the registry is not enough to trigger a deployment.


### Secrets

There are a small number of sensitive details that need to be carefully handled, including the SSL cert and key for Nginx, possibly TLS client and server certs for Consul, and a license key for the CMS. The application operators expect Mariposa to offer features to simplify that process.

Secrets can be injected by Mariposa into containers via instance metadata. These can be accessed via ContainerPilot preStart or the application.

In hardened environments the instance metadata could be a one-time token for something like Vault. The scheduler would need to coordinate with w/ the external secrets management server though (maybe for a post-MVP).

Jenkins would generate secrets as part of the job for the dev environment, add those to the manifest’s metadata (this isn’t in the manifest file itself!). This allows the dev credentials be ephemeral for each deployment.

For production metadata, the operators can add those secrets to Mariposa out-of-band from the CI server. This way Jenkins never gets the production secrets. It only has the manifest not the metadata for production. (Although in practice in has the keys to the universe until RBAC v2.)

### Versioning of components

- The source code is versioned in the GitHub repo and tagged, including the manifest templates
- The container image is stored and tagged in a Docker registry
- The service or project manifest version details the infrastructure configuration, including not just the image and tag for the container, the open ports, RAM allocation/package size, and other details.
- Should metadata be versioned? Or is it better not to version this so that it’s easier to destroy metadata and rotate creds?
