# jupiter.example.com: what it is and development workflow

Story details include:

- project basics
- deployment and updates

The trivial [jupiter.example.com](./jupiter-example-com.md) application consists of a number of stateless containers (or containers whose state can be easily reconstructed) running as a tiered application of Nginx fronting a PHP/Apache web content templating system. The container relationships are managed by ContainerPilot and Consul. The app is further fronted by a CDN service that discovers the active Nginx instances using CNS.

Updates to the application and its content are done by building and staging new containers before deploying them to replace the existing containers.

The application runs three instances of each tier in a single data center, but ideally it would have multiple data centers (perhaps with just one instance in each tier of each data center). The only application component that would need to peer across data centers is Consul (though this is uncommon and most apps would have other components that would need to peer across data centers).

There are a small number of sensitive details that need to be carefully handled, including the SSL cert and key for Nginx, possibly TLS client and server certs for Consul, and a license key for the CMS. The application operators expect Mariposa to offer features to simplify that process.

The code is hosted in GitHub and the images in a privately operated repository. The operators use Jenkins to automate the process of building new containers for each pull request and branch in the origin repo. After building the containers, Jenkins creates a new Mariposa project in which to stage it, or identifies and re-uses an existing project. The project names follow a predictable pattern based on the PR number of branch name (`<repo-name>-pr-<PR number>` or `<repo-name>-branch-<branch name>`), that allows the easy human and machine recognition of the relationship between projects in GitHub and Mariposa. The live, production project is named `<repo-name>-prod`, which cannot be matched by the naming logic the operators have defined in Jenkins.

Despite the automation for continuous build and deploy of the staging projects, the deploys to the live site require manual promotion. After reviewing the relevant staging site and any logs generated by Jenkins, the operators merge a PR to master, triggering a new build of master that is tagged with the date and time of the build (the builds collect resources beyond the scope of the git repo, so the git SHA is not a consistent identifier). Jenkins stages that build in a project named `<repo-name>-branch-<branch name>` (using the same automation logic from above). After testing and previewing that final staging build, the operators can choose to manually deploy that build to the live site.

At this point, we face a number of questions that need further exploration specific to the image tag:

1. How do we indicate which tag is the correct tag to run in production?
2. How can we do that in a way that minimizes the risk of errors due to copypasta or other mistakes that are inherent in a manual process?
3. Because tags are not immutable, we must assume that at some point the tag for an image in the registry will no longer be the same as the currently deployed image. How do we avoid accidental deploys if Triton/Mariposa scale up or replace an instance?
4. How do we address the questions above while keeping the scope and complexity small?

Additionally, we should consider what components are versioned where:

1. The service or project manifest details the infrastructure configuration, including not just the image and tag for the container, the open ports, RAM allocation/package size, and other details.
2. The image is assumed to be stored and tagged in a Docker registry, and it's further assumed that it's built from code in a versioned repo (GitHub or others)
3. The service or project manifest may also be ingested from (and versioned in) a code repo (GitHub or others)

Additional reading:

- [Running jupiter.example.com in multiple data centers](./jupiter-example-com-multi-dc.md)
- [Health-checking, monitoring, and scaling jupiter.example.com](./jupiter-example-com-monitoring-and-health.md)
- [Creating, copying, and moving projects like microsite.jupiter.example.com](./microsite-jupiter-example-com.md)
